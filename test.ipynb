{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Map predictions to stance categories if necessary\u001b[39;00m\n\u001b[0;32m     75\u001b[0m stance_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisagree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscuss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m3\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munrelated\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m---> 76\u001b[0m predicted_stances \u001b[38;5;241m=\u001b[39m [stance_mapping[class_id] \u001b[38;5;28;01mfor\u001b[39;00m class_id \u001b[38;5;129;01min\u001b[39;00m best_predictions]\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Save predictions to CSV\u001b[39;00m\n\u001b[0;32m     79\u001b[0m merged_test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted_Stance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_stances\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "DATASET_PATH = \"./fake/\"\n",
    "test_bodies_path = os.path.join(DATASET_PATH, 'test_bodies.csv')\n",
    "test_stances_path = os.path.join(DATASET_PATH, 'test_stances_unlabeled.csv')\n",
    "\n",
    "# Load test data\n",
    "test_bodies = pd.read_csv(test_bodies_path)\n",
    "test_stances = pd.read_csv(test_stances_path)\n",
    "\n",
    "# Ensure 'Body ID' column is of the same type in both dataframes\n",
    "test_stances['Body ID'] = test_stances['Body ID'].astype(str)\n",
    "test_bodies['Body ID'] = test_bodies['Body ID'].astype(str)\n",
    "\n",
    "# Merge the test datasets on 'Body ID'\n",
    "merged_test_data = pd.merge(test_stances, test_bodies[['Body ID', 'articleBody']], on='Body ID', how='left')\n",
    "\n",
    "# Extract headlines and bodies\n",
    "headlines_test = merged_test_data['Headline'].fillna('')\n",
    "bodies_test = merged_test_data['articleBody'].fillna('')\n",
    "\n",
    "# Tokenizer settings (same as used for training)\n",
    "max_features = 2000\n",
    "MAX_SEQUENCE_LENGTH_HEADLINE = 16\n",
    "MAX_SEQUENCE_LENGTH_BODY = 48\n",
    "embedding_dim = 50\n",
    "\n",
    "# Tokenize and pad the headlines and bodies\n",
    "tokenizer_headline = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer_body = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer_headline.fit_on_texts(headlines_test)\n",
    "tokenizer_body.fit_on_texts(bodies_test)\n",
    "\n",
    "encoded_docs_headline_test = tokenizer_headline.texts_to_sequences(headlines_test)\n",
    "padded_docs_headline_test = pad_sequences(encoded_docs_headline_test, maxlen=MAX_SEQUENCE_LENGTH_HEADLINE, padding='post')\n",
    "\n",
    "encoded_docs_body_test = tokenizer_body.texts_to_sequences(bodies_test)\n",
    "padded_docs_body_test = pad_sequences(encoded_docs_body_test, maxlen=MAX_SEQUENCE_LENGTH_BODY, padding='post')\n",
    "\n",
    "# Initialize variables to keep track of the best model\n",
    "best_model_path = None\n",
    "best_accuracy = 0\n",
    "best_predictions = None\n",
    "\n",
    "# Iterate over the model files\n",
    "for model_file in os.listdir(DATASET_PATH):\n",
    "    if model_file.endswith('.hdf5'):\n",
    "        model_path = os.path.join(DATASET_PATH, model_file)\n",
    "        \n",
    "        # Load the model\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.predict([padded_docs_headline_test, padded_docs_body_test])\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Calculate accuracy (optional: if you have the true labels)\n",
    "        # accuracy = np.mean(predicted_classes == true_labels) # If true_labels are available\n",
    "        \n",
    "        # For now, let's assume the latest model is the best based on the validation accuracy\n",
    "        current_accuracy = float(model_file.split('-')[1].replace('.hdf5', ''))\n",
    "        if current_accuracy > best_accuracy:\n",
    "            best_accuracy = current_accuracy\n",
    "            best_model_path = model_path\n",
    "            best_predictions = predicted_classes\n",
    "\n",
    "# Map predictions to stance categories if necessary\n",
    "stance_mapping = {0: 'agree', 1: 'disagree', 2: 'discuss', 3: 'unrelated'}\n",
    "predicted_stances = [stance_mapping[class_id] for class_id in best_predictions]\n",
    "\n",
    "# Save predictions to CSV\n",
    "merged_test_data['Predicted_Stance'] = predicted_stances\n",
    "merged_test_data.to_csv(os.path.join(DATASET_PATH, 'test_predictions.csv'), index=False)\n",
    "\n",
    "print(f\"Testing complete with the best model '{best_model_path}'. Predictions saved to 'test_predictions.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
