{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.10.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E02sBfQBb1k6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The system cannot find the path specified.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[WinError 1314] A required privilege is not held by the client: '/kaggle/input' -> '..\\\\input'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(KAGGLE_WORKING_PATH, \u001b[38;5;241m0o777\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m   \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKAGGLE_INPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m..\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_is_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 1314] A required privilege is not held by the client: '/kaggle/input' -> '..\\\\input'"
          ]
        }
      ],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'glove50d:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F911310%2F1544853%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240907%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240907T170146Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D24743ef0d8c855b54c7ca73108048c6094cd8f39b8edf2a4663f0b50ddfe5e4a97bc672766ff2353fa1fe61a1274661744316ae8674ae4bbd9c5aaf49b00f558658f39c6223649cd8edaa2a90b53207063cc40167c86f66875116ffcc658615cd06f4d8459ccc993e2871e7995ada0b9a9a7fa5b90b8050c39ed043786df1e7777f52b323e74e27c1415853dab951200f712c6d9b8f771b3e7bb44b6c068ed32bf6ca0b11201995f1723c51ff1ffdf312c15b761bb04868171f85c36d0d13a955782a93741591faf59a98e682690a600934a54294b7e2dfd85e8dac9240ca0e9850ea6af80eacb76c52f35e4e83ae0324ed2ed836148da54137a41c350837fd7,fake-news-challenge:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1249857%2F2084561%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240907%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240907T170146Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D08fe71600e5dabff5bfaebb31f72e5bd4b7b03a9083390fc3b304dc7d90c70422b7440a46a4a0457b9ec725f0d749e2d3483a6f59093e61379227a28d9ebbe41af8f46d7497fa5dd9de172becdcff0b2611a0e53dc0d73eaa9f7382ae60468a7ac5d0b1208702b33b99cbc96ac0839fc32db638d4fd3ef2df44faf90e9a6cc8b5580430997e516fb0d50fbd5e6a2982226861168e9dc36938b6300383531481e15dfdd2ba7e8112fdabfd12a7bac9037d76a0708fd3a18485b286bce1e4c2bb2149026df4c970fb2eb7696909ae5edd9d2aa231ad68f189ea68475e52554ef9192eecfbd8fa196bda6385017c18ca685d7486ff0c42cd63ec9308a0c41bad439'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_B7jvIjb1lE"
      },
      "source": [
        "# Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ihEQUwsb1lH"
      },
      "source": [
        "1. We are given a dataset consisting of two csv files train_bodies.csv which contains the set of news articles bodies,while train-stances.csv resembles the articles for each of these bodies being identified using the body id.\n",
        "\n",
        "2. After training from these samples we need to detect whether the given headline agrees,disagrees,discusses,unrelated with the body id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK5fNhcdb1lJ"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wArZWkJb1lK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical,plot_model\n",
        "\n",
        "from keras.models import Input,Model,Sequential\n",
        "from keras.layers import LSTM,Embedding,Dropout,Activation,Reshape,Dense,GRU,Add,Flatten,concatenate,Bidirectional\n",
        "\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.utils import to_categorical,plot_model\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNd8m70cb1lL"
      },
      "source": [
        "# Dataset understanding\n",
        "The train_bodies contain the entries for the body id and associated article Body\n",
        "The train_stances contain the entries for the headlines associated with the particular body id and its labelled stance\n",
        "One body present in train_bodies can have multiple associated headlines present in train_stances and it's corresponding stance label\n",
        "1683 :- Number of article Body present\n",
        "49972 number of total headlines present for the 1683 different article body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYm3QMfXb1lM"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "**train_bodies.csv** contains body id and article body for training  \n",
        "**train_stances.csv** contains headlines corresponding to body id and associated labelled stance with it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eLPtsg0b1lO",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"../input/fake-news-challenge/\"\n",
        "\n",
        "train_bodies = pd.read_csv(os.path.join(DATASET_PATH,'train_bodies.csv'))\n",
        "# train_bodies.head()\n",
        "train_stance = pd.read_csv(os.path.join(DATASET_PATH,'train_stances.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPwSGCfRb1lP"
      },
      "source": [
        "# Combining the CSV\n",
        "\n",
        "I am preparing a final csv in each row will correspond to a unique entry\n",
        "i.e each row will correspond to a unique combination of headline,bodyid and article body\n",
        "\n",
        "The above is needed for making simplicity in further data preparation steps we need to execute\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCUeX57gb1lQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Run commented code to combine the two csv file{train_bodies.csv,train_stances.csv} into data_combined.csv file\n",
        "from tqdm.notebook import tqdm\n",
        "count=0\n",
        "for i in tqdm(range(train_stance.shape[0])):\n",
        "    for j in range(train_bodies.shape[0]):\n",
        "        if train_bodies.loc[j,'Body ID']==train_stance.loc[i,'Body ID']:\n",
        "            train_stance.loc[i,'articleBody'] = train_bodies.loc[j,'articleBody']\n",
        "\n",
        "\n",
        "train_stance.to_csv(os.path.join(os.getcwd(),'data_combined.csv'),index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtKpIAQcb1lR",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(os.path.join(os.getcwd(),'data_combined.csv'))#generated from Fake News stanford.ipynb\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTYKAkakb1lS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data['stance_cat'] = data['Stance'].map({'agree':0,'disagree':1,'discuss':2,'unrelated':3}).astype(int)\n",
        "data['Stance'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HlmD8RTb1lT",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "corpus = np.r_[data['Headline'].values,data['articleBody'].values]\n",
        "print(49972*2)\n",
        "print(len(corpus)) # first 49972 contains the Headline and next 49972 contains the articleBody\n",
        "\n",
        "vocabulary = []\n",
        "for sentence in corpus:\n",
        "    vocabulary.extend(sentence.split(' '))\n",
        "\n",
        "vocabulary = list(set(vocabulary))\n",
        "vocab_length = len(vocabulary)\n",
        "print(\"Vocabulary Length is {0}\".format(vocab_length))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAOf2a-qb1lT"
      },
      "source": [
        "## Model Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fF0YSiLtb1lU",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "max_features = 5000\n",
        "MAX_NB_WORDS = 24000\n",
        "EMBEDDING_DIM = 50\n",
        "MAX_SEQUENCE_LENGTH = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5NZx8Mib1lU"
      },
      "source": [
        "## Creating Embedding Matrix For Headline and Body\n",
        "\n",
        "We create Emebdding Matrix for headline and Body to be served as a first layer of Deep learning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS6MqXjQb1lV",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "GLOVE_DIR = \"../input/glove50d/\"\n",
        "def setup_embedding_index():\n",
        "    embedding_index=dict()\n",
        "    f = open(os.path.join(GLOVE_DIR,\"glove.6B.50d.txt\"),encoding='utf-8')\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.array(values[1:],dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "    f.close()\n",
        "    return embedding_index\n",
        "embeddings_index = setup_embedding_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q34I6U5vb1lV"
      },
      "source": [
        "## Padding headline and body\n",
        "\n",
        "We pad the headline into length of 16 as headline is of shorter length and body into length of 48 as observed best performing parameter for body is 48."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psx0b1Vmb1lW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer_headline = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer_headline.fit_on_texts(data.loc[:,'Headline'].values)\n",
        "vocab_headline_length = len(tokenizer_headline.word_index)+1\n",
        "\n",
        "encoded_docs_headline = tokenizer_headline.texts_to_sequences(data.loc[:,'Headline'])\n",
        "padded_docs_headline = pad_sequences(encoded_docs_headline, maxlen=16, padding='post')\n",
        "\n",
        "print(vocab_headline_length)\n",
        "word_index_headline = tokenizer_headline.word_index\n",
        "\n",
        "NUM_WORDS_HEADLINE = vocab_headline_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCocS3NVb1lW",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer_body = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer_body.fit_on_texts(data.loc[:,'articleBody'].values)\n",
        "vocab_body_length = len(tokenizer_body.word_index)+1\n",
        "\n",
        "encoded_docs_body = tokenizer_body.texts_to_sequences(data.loc[:,'articleBody'])\n",
        "padded_docs_body = pad_sequences(encoded_docs_body, maxlen=48, padding='post')\n",
        "\n",
        "print(vocab_body_length)\n",
        "word_index_body = tokenizer_body.word_index\n",
        "\n",
        "\n",
        "NUM_WORDS_BODY = vocab_body_length\n",
        "print(NUM_WORDS_BODY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grjz75APb1lX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_matrix_headline = np.zeros((NUM_WORDS_HEADLINE, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in tokenizer_headline.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_headline[i] = embedding_vector\n",
        "dims = len(embedding_matrix_headline[0])\n",
        "\n",
        "print(dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b454x_8Bb1lX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_matrix_body = np.zeros((NUM_WORDS_BODY, EMBEDDING_DIM))\n",
        "\n",
        "for word, i in tokenizer_body.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_body[i] = embedding_vector\n",
        "dims = len(embedding_matrix_body[0])\n",
        "\n",
        "print(dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6fGm8ewb1lX",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(padded_docs_headline.shape)\n",
        "print(padded_docs_body.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee8VO4VDb1lY"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym9O7jrHb1lY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "input_headline = Input(shape=16,name='input_headline')\n",
        "embedding_layer_headline = Embedding(input_dim = vocab_headline_length,output_dim = 50,\n",
        "                                     weights=[embedding_matrix_headline],\n",
        "                                     input_length = 16,trainable=True)(input_headline)\n",
        "\n",
        "# lstm_headline = LSTM(units=16)(embedding_layer_headline)\n",
        "\n",
        "input_body = Input(shape=48,name='input_body')\n",
        "embedding_layer_body = Embedding(input_dim = vocab_body_length,output_dim = 50,weights = [embedding_matrix_body],\n",
        "                                 input_length=48,trainable = True)(input_body)\n",
        "lstm_body = LSTM(units=48)(embedding_layer_body)\n",
        "\n",
        "addition_layer = concatenate([embedding_layer_headline,embedding_layer_body],axis=1)\n",
        "\n",
        "# addition_layer = concatenate([lstm_headline,lstm_body],axis=1)\n",
        "lstm = LSTM(units=64,)(addition_layer)\n",
        "drop = Dropout(0.25)(lstm)\n",
        "# dense = Dense(64,activation='relu')(drop)\n",
        "# flatten = Flatten()(addition_layer)\n",
        "\n",
        "output = Dense(4,activation='sigmoid')(drop)\n",
        "\n",
        "model = Model(inputs=[input_headline,input_body],outputs=output)\n",
        "# from keras.optimizers import SGD\n",
        "# sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# model.compile(loss = \"categorical_crossentropy\", optimizer = sgd,metrics = ['accuracy'])\n",
        "\n",
        "model.compile(optimizer = 'adam',loss ='categorical_crossentropy',metrics = ['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZo-2riGb1lZ",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXhvbM5ob1lZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "plot_model(model, to_file='model_glove_lstm.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT0UZnyjb1lZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "padded_docs_headline_train = padded_docs_headline[:int(len(padded_docs_headline)*0.9),:]\n",
        "padded_docs_headline_test = padded_docs_headline[int(len(padded_docs_headline)*0.9):,:]\n",
        "\n",
        "padded_docs_body_train = padded_docs_body[:int(len(padded_docs_body)*0.9),:]\n",
        "padded_docs_body_test = padded_docs_body[int(len(padded_docs_body)*0.9):,:]\n",
        "\n",
        "labels = to_categorical(data.loc[:,'stance_cat'])\n",
        "\n",
        "labels_train = labels[:int(len(labels)*0.9),:]\n",
        "labels_test = labels[int(len(labels)*0.9):,:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik9aNrvlb1la"
      },
      "source": [
        "## Creating Checkpoints\n",
        "\n",
        "For saving the latest model trained after every epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ8BlWZjb1la",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# MODELS_DIR = os.path.join(\"/home/abhinav/fake_news_challenge/model/glove_lstm\")\n",
        "filepath = os.path.join(os.getcwd(),\"{epoch:02d}-{val_accuracy:.2f}.hdf5\")\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irUjIGeyb1la"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV1YNVhbb1lb",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_history = model.fit([padded_docs_headline_train,padded_docs_body_train],labels_train,epochs=40,shuffle=True,verbose=1,\n",
        "                          validation_data=([padded_docs_headline_test,padded_docs_body_test],labels_test),\n",
        "                                          callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9r2n3xfb1lc"
      },
      "source": [
        "## Model Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhsvCT19b1lc",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
        "ax1.plot(model_history.history['loss'], color='b', label=\"Training loss\")\n",
        "ax1.plot(model_history.history['val_loss'], color='r', label=\"validation loss\")\n",
        "ax1.set_xticks(np.arange(1, 40, 1))\n",
        "ax1.set_yticks(np.arange(0, 1, 0.1))\n",
        "\n",
        "ax2.plot(model_history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax2.plot(model_history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "ax2.set_xticks(np.arange(1, 40, 1))\n",
        "\n",
        "legend = plt.legend(loc='best', shadow=True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_okXSmFb1lc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Glove+LSTM  95.08% Acc",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
